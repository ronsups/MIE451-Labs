{"nbformat_minor": 2, "cells": [{"source": "## Predefined Spark varibles sc, spark and sqlContext", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "sqlContext", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Parallelize a python data structure", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "myRDD = sc.parallelize(range(1000), 10)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "myRDD.getNumPartitions()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "myRDD.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Rename RDD and set persistency level", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.storagelevel import *\nmyRDD.setName(\"myRDD\").persist(StorageLevel.MEMORY_AND_DISK_SER) ", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "tmpMyRDD = myRDD.map(lambda x: x*2).filter(lambda x: x < 10).collect()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check UI -> Storage to see myRDD\n\n## How partitions work?", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "allNums = myRDD.glom().collect()", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "len(allNums)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "allNums[0:2]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Loading twitter dataset", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import *\nimport time", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "tweetsCSV = sc.textFile(\"wasb://mie451datasets@mie451files.blob.core.windows.net/tweets2009-06-0115.csv\", 20)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "tweetsCSV.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "tweetsCSV.getNumPartitions()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "def isEnglish(s):\n    try:\n        s.encode('ascii')\n    except UnicodeEncodeError:\n        return False\n    else:\n        return True\n\n# RDD Transformations: parse the data in tweetsCSV\\n\",\ntweets = tweetsCSV.filter(lambda s: isEnglish(s)).map(lambda s: s.split(\"\\t\")).filter(lambda s: s[0] != \"date\" and len(s) == 3).map(lambda s:(str(s[0]), str(s[1]), str(s[2])))", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "tweets.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "tweets.getNumPartitions()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Unoptimized Code", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\ntweetCounts = tweets.groupBy(lambda x: x[1]).map(lambda x: (x[0], len(x[1]))).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\ntweetCounts = tweets.groupBy(lambda x: x[1]).map(lambda x: (x[0], x[1].data[0])).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\ntweetCounts = tweets.groupBy(lambda x: x[1]).map(lambda x: (x[0], x[1].data[-1])).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Better partitioning", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "partitioned = tweets.groupBy(lambda x: x[1]).partitionBy(20)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\npartitioned.map(lambda x: (x[0], len(x[1]))).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\npartitioned.map(lambda x: (x[0], x[1].data[0])).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\npartitioned.map(lambda x: (x[0], x[1].data[-1])).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Using Cache", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "groupedRDD = tweets.groupBy(lambda x: x[1])\ngroupedRDD.cache()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\ntweetCounts = groupedRDD.map(lambda x: (x[0], len(x[1]))).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\ntweetCounts = groupedRDD.map(lambda x: (x[0], x[1].data[0])).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "time1 = time.time()\ntweetCounts = groupedRDD.map(lambda x: (x[0], x[1].data[-1])).collect()\ntime2 = time.time()\nprint((time2-time1)*1000.0)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## DataFrames", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Create schema for dataframe\ntweetsSchema = StructType([StructField(\"date\", StringType(), False), \n                           StructField(\"user\", StringType(), False), \n                           StructField(\"tweet\", StringType(), False)])\n\n# Create data frame\ntweetsDF = sqlContext.createDataFrame(tweets, tweetsSchema)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "tweetsDF", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "tweetsDF.printSchema()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "tweetsDF.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Working with SQL", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "tweetsDF.registerTempTable(\"tweets\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "sqlContext.sql(\"SELECT * FROM tweets LIMIT 5\").show()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "summarizedDF = sqlContext.sql(\"SELECT user, COUNT(1) AS tweetCount from tweets GROUP BY user\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "summarizedDF.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## using SparkSQL functions", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import pyspark.sql.functions as sf\ntweetsDF.groupBy(\"user\").agg(sf.count(\"*\").alias(\"tweetCount\")).select(\"user\", \"tweetCount\").show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Joining", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#.join default it INNER JOIN\ncombined = tweetsDF.join(summarizedDF, tweetsDF.user == summarizedDF.user).select(\"date\", tweetsDF.user, \"tweet\", \"tweetCount\")\ncombined.show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### and as SQL query:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "summarizedDF.registerTempTable(\"summarized\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "sqlContext.sql(\"\"\"\nSELECT tweets.date, tweets.user, tweets.tweet, summarized.tweetCount\nFROM tweets INNER JOIN summarized on tweets.user = summarized.user\n\"\"\").show()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Caching", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# Result format (useDisk, useMemory, useOffHeap, deserialized, replication)\nsummarizedDF.storageLevel", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "str(summarizedDF.storageLevel)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "summarizedDF.cache() # equivalent to .persist(MEMORY_AND_DISK)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "summarizedDF.storageLevel", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "str(summarizedDF.storageLevel)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Extracting Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "rows = tweetsDF.take(5) # or collect() for the whole dataframe\nrows", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rows[0]", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rows[0].user", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rows[0].asDict()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Alternative: Export to Pandas", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "pdDF = tweetsDF.limit(5).toPandas()\npdDF", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "type(pdDF)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Saving processed dataframe", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# format can be also json, ...\ntweetsDF.write.format('parquet').save(\"/path/to/output\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "loadedDF = sqlContext.read.parquet(\"/path/to/output\")", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}