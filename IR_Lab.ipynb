{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations:\n",
    "* Download the provided dataset **DSS_Fall2017_Assign1.zip** and extract it to a local folder (e.g., C:\\DSS_Fall2017_Assign1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MATERIALS_DIR = r\"C:\\DSS_Fall2017_Assign2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import necessary libraries: whoosh(for IR) and os,shutil(for working with files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh import index, writing\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import *\n",
    "from whoosh.qparser import QueryParser\n",
    "import os, os.path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCUMENTS_DIR = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\")\n",
    "INDEX_DIR = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\index1\")\n",
    "QUER_FILE = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\topics\\air.topics\")\n",
    "QRELS_FILE = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\qrels\\air.qrels\")\n",
    "OUTPUT_FILE = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\myres\")\n",
    "TREC_EVAL = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\trec_eval\\trec_eval.exe\")\n",
    "INDEX_DIR2 = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\index2\")\n",
    "OUTPUT_FILE2 = os.path.join(MATERIALS_DIR, r\"DSS_Fall2017_Assign2\\lab1-q1-test-collection\\myres2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, define a Schema for the index\n",
    "mySchema = Schema(file_path = ID(stored=True),\n",
    "                  file_content = TEXT(analyzer = RegexTokenizer()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if index exists - remove it\n",
    "if os.path.isdir(INDEX_DIR):\n",
    "    shutil.rmtree(INDEX_DIR)\n",
    "\n",
    "# create the directory for the index\n",
    "os.makedirs(INDEX_DIR)\n",
    "\n",
    "# create index\n",
    "myIndex = index.create_in(INDEX_DIR, mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 10DF-697A\n",
      "\n",
      " Directory of C:\\DSS_Fall2017_Assign2\\DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\n",
      "\n",
      "09/29/2017  12:39 PM    <DIR>          .\n",
      "09/29/2017  12:39 PM    <DIR>          ..\n",
      "09/29/2017  12:39 PM            19,703 email01\n",
      "09/29/2017  12:39 PM            19,844 email02\n",
      "09/29/2017  12:39 PM            19,059 email03\n",
      "09/29/2017  12:39 PM            19,548 email04\n",
      "09/29/2017  12:39 PM            16,693 email05\n",
      "09/29/2017  12:39 PM            20,332 email06\n",
      "09/29/2017  12:39 PM            19,995 email07\n",
      "09/29/2017  12:39 PM            18,075 email08\n",
      "09/29/2017  12:39 PM            18,621 email09\n",
      "09/29/2017  12:39 PM            19,884 email10\n",
      "09/29/2017  12:39 PM            18,336 email14\n",
      "              11 File(s)        210,090 bytes\n",
      "               2 Dir(s)  66,214,862,848 bytes free\n"
     ]
    }
   ],
   "source": [
    "# First, lets review the documents in our sample dataset\n",
    "!dir $DOCUMENTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first we build a list of all the full paths of the files in DOCUMENTS_DIR\n",
    "filesToIndex = []\n",
    "for root, dirs, files in os.walk(DOCUMENTS_DIR):\n",
    "    filePaths = [os.path.join(root, fileName) for fileName in files if not fileName.startswith('.')]\n",
    "    filesToIndex.extend(filePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\DSS_Fall2017_Assign2\\DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\\email01\n",
      "C:\\DSS_Fall2017_Assign2\\DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\\email02\n",
      "C:\\DSS_Fall2017_Assign2\\DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\\email03\n",
      "C:\\DSS_Fall2017_Assign2\\DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\\email04\n",
      "C:\\DSS_Fall2017_Assign2\\DSS_Fall2017_Assign2\\lab1-q1-test-collection\\documents\\email05\n"
     ]
    }
   ],
   "source": [
    "# print the first 5 paths to make sure it worked\n",
    "print(\"\\n\".join(filesToIndex[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files: 11\n"
     ]
    }
   ],
   "source": [
    "# count files to index\n",
    "print(\"number of files:\", len(filesToIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already indexed: 1\n",
      "done indexing.\n"
     ]
    }
   ],
   "source": [
    "# open writer\n",
    "myWriter = writing.BufferedWriter(myIndex, period=20, limit=1000)\n",
    "\n",
    "try:\n",
    "    # write each file to index\n",
    "    for docNum, filePath in enumerate(filesToIndex):\n",
    "        with open(filePath, \"r\") as f:\n",
    "            fileContent = f.read()\n",
    "            myWriter.add_document(file_path = filePath,\n",
    "                                  file_content = fileContent)\n",
    "            \n",
    "            if (docNum % 1000 == 0):\n",
    "                print(\"already indexed:\", docNum+1)\n",
    "    print(\"done indexing.\")\n",
    "\n",
    "finally:\n",
    "    # save the index\n",
    "    myWriter.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a query parser for the field \"file_content\" in the index\n",
    "myQueryParser = QueryParser(\"file_content\", schema=myIndex.schema)\n",
    "mySearcher = myIndex.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email01 0 2.6746417187049216\n"
     ]
    }
   ],
   "source": [
    "# run a sample query for the phrase \"item\"\n",
    "sampleQuery = myQueryParser.parse(\"item\")\n",
    "sampleQueryResults = mySearcher.search(sampleQuery, limit=None)\n",
    "\n",
    "# inspect the result:\n",
    "# for each document print the rank and the score\n",
    "for (docnum, result) in enumerate(sampleQueryResults):\n",
    "    score = sampleQueryResults.score(docnum)\n",
    "    fileName = os.path.basename(result[\"file_path\"])\n",
    "    print(fileName, docnum, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using TREC_EVAL\n",
    "In order to evaluate our results we will use a topic file - a list of topics we use to evaluate our IR system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 ducks\n",
      "02 ig nobel prizes\n",
      "03 mathematics\n",
      "04 flowing hair\n",
      "05 music\n",
      "06 AIR TV\n"
     ]
    }
   ],
   "source": [
    "# print the topic file\n",
    "!cat $QUER_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare our evaluate our results with a set of judged results(qrels file) using TREC_EVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 0 email01 0\n",
      "01 0 email02 0\n",
      "01 0 email03 0\n",
      "01 0 email04 1\n",
      "01 0 email05 1\n",
      "01 0 email06 1\n",
      "01 0 email07 0\n",
      "01 0 email08 0\n",
      "01 0 email09 0\n",
      "01 0 email10 0\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 lines in the qrels file\n",
    "!head -n 10 $QRELS_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a file with our results accoring to TREC_EVAL format (see assignment PDF for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load topic file - a list of topics(search phrases) used for evalutation\n",
    "topicsFile = open(QUER_FILE,\"r\")\n",
    "topics = topicsFile.read().splitlines()\n",
    "\n",
    "# create an output file to which we'll write our results\n",
    "outputTRECFile = open(OUTPUT_FILE, \"w\")\n",
    "\n",
    "# for each evaluated topic:\n",
    "# build a query and record the results in the file in TREC_EVAL format\n",
    "for topic in topics:\n",
    "    topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
    "    topicQuery = myQueryParser.parse(topic_phrase)\n",
    "    topicResults = mySearcher.search(topicQuery, limit=None)\n",
    "    for (docnum, result) in enumerate(topicResults):\n",
    "        score = topicResults.score(docnum)\n",
    "        outputTRECFile.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
    "\n",
    "# close the topic and results file\n",
    "outputTRECFile.close()\n",
    "topicsFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use TREC_EVAL to compare our results with the provided qrels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!$TREC_EVAL -q $QRELS_FILE $OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b. Evaluating different configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Is it empty?\n",
    "print(\"Index is empty?\", myIndex.is_empty())\n",
    "\n",
    "# How many files indexed?\n",
    "print(\"Number of indexed files:\", myIndex.doc_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a reader object on the index\n",
    "myReader = myIndex.reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print first 5 indexed documents\n",
    "[(docnum, doc_dict) for (docnum, doc_dict) in myReader.iter_docs()][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list indexed terms for field \"file_content\"\n",
    "[term for term in myReader.field_terms(\"file_content\")][1000:1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how many terms do we have?\n",
    "print(myReader.field_length(\"file_content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many documents have the phares \"bit\", blob\"\n",
    "#   in the field \"file_content\"?\n",
    "print(\"# docs with 'bit'\", myReader.doc_frequency(\"file_content\", \"bit\"))\n",
    "print(\"# docs with 'are'\", myReader.doc_frequency(\"file_content\", \"are\"))\n",
    "print(\"# docs with 'get'\", myReader.doc_frequency(\"file_content\", \"get\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we start with basic tokenizer\n",
    "tokenizer = RegexTokenizer()\n",
    "[token.text for token in tokenizer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we might want use stemming:\n",
    "stmAnalyzer = RegexTokenizer() | StemFilter()\n",
    "[token.text for token in stmAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We probably want to lower-case it\n",
    "# so we add LowercaseFilter\n",
    "stmLwrAnalyzer = RegexTokenizer() | LowercaseFilter() | StemFilter()\n",
    "[token.text for token in stmLwrAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we probably want to ignore words like \"we\", \"are\", \"with\" when we index files\n",
    "# so we add StopFilter to filter stop words\n",
    "stmLwrStpAnalyzer = RegexTokenizer() | LowercaseFilter() | StopFilter() | StemFilter()\n",
    "[token.text for token in stmLwrStpAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we also probably want to break phrases like \"whoosh.analysis\" into \"whoosh\" and \"analysis\"\n",
    "# so we add IntraWordFilter\n",
    "stmLwrStpIntraAnalyzer = RegexTokenizer() | LowercaseFilter() | IntraWordFilter() | StopFilter() | StemFilter()\n",
    "[token.text for token in stmLwrStpIntraAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the new analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mySchema2 = Schema(file_path = ID(stored=True),\n",
    "                   file_content = TEXT(analyzer = stmLwrStpIntraAnalyzer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if index exists - remove it\n",
    "if os.path.isdir(INDEX_DIR2):\n",
    "    shutil.rmtree(INDEX_DIR2)\n",
    "\n",
    "# create the directory for the index\n",
    "os.makedirs(INDEX_DIR2)\n",
    "\n",
    "# create index or open it if already exists\n",
    "myIndex2 = index.create_in(INDEX_DIR2, mySchema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open writer\n",
    "myWriter2 = writing.BufferedWriter(myIndex2, period=20, limit=1000)\n",
    "\n",
    "try:\n",
    "    # write each file to index\n",
    "    for docNum, filePath in enumerate(filesToIndex):\n",
    "        with open(filePath, \"r\") as f:\n",
    "            fileContent = f.read()\n",
    "            myWriter2.add_document(file_path = filePath,\n",
    "                                  file_content = fileContent)\n",
    "            \n",
    "            if (docNum % 1000 == 0):\n",
    "                print(\"already indexed:\", docNum+1)\n",
    "    print(\"done indexing.\")\n",
    "\n",
    "finally:\n",
    "    # save the index\n",
    "    myWriter2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a query parser for the field \"file_content\" in the index\n",
    "myQueryParser2 = QueryParser(\"file_content\", schema=myIndex2.schema)\n",
    "mySearcher2 = myIndex2.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load topic file - a list of topics(search phrases) used for evalutation\n",
    "topicsFile = open(QUER_FILE,\"r\")\n",
    "topics = topicsFile.read().splitlines()\n",
    "\n",
    "# create an output file to which we'll write our results\n",
    "outputTRECFile2 = open(OUTPUT_FILE2, \"w\")\n",
    "\n",
    "# for each evaluated topic:\n",
    "# build a query and record the results in the file in TREC_EVAL format\n",
    "for topic in topics:\n",
    "    topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
    "    topicQuery = myQueryParser2.parse(topic_phrase)\n",
    "    topicResults = mySearcher2.search(topicQuery, limit=None)\n",
    "    for (docnum, result) in enumerate(topicResults):\n",
    "        score = topicResults.score(docnum)\n",
    "        outputTRECFile2.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
    "\n",
    "# close the topic and results file\n",
    "outputTRECFile2.close()\n",
    "topicsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!$TREC_EVAL -q $QRELS_FILE $OUTPUT_FILE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let count the same words again\n",
    "myReader2 = myIndex2.reader()\n",
    "print(\"# docs with 'bit'\", myReader2.doc_frequency(\"file_content\", \"bit\"))\n",
    "print(\"# docs with 'are'\", myReader2.doc_frequency(\"file_content\", \"are\"))\n",
    "print(\"# docs with 'get'\", myReader2.doc_frequency(\"file_content\", \"get\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Can you explain the differences? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK's stemmers and lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download required resources\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll compare two stemmers and a lemmatizer\n",
    "lrStem = LancasterStemmer()\n",
    "sbStem = SnowballStemmer(\"english\")\n",
    "wnLemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a list of words to compare the stemmers on\n",
    "listWords = [\"going\", \"saying\", \"minimize\", \"maximum\", \n",
    "             \"meeting\", \"files\", \"tries\", \"is\", \"are\", \"beautiful\",\n",
    "             \"summarize\", \"better\", \"dogs\", \"phenomena\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in listWords:\n",
    "    print(\"%15s %15s %15s %15s\" % (lrStem.stem(word),\n",
    "                                   sbStem.stem(word),\n",
    "                                   wnLemm.lemmatize(word),\n",
    "                                   wnLemm.lemmatize(word, 'v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use NLTK stemmers / lemmatizers in Whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dont change this! Use it as-is in your code\n",
    "# This filter will run for both the index and the query\n",
    "from whoosh.analysis import Filter\n",
    "class CustomFilter(Filter):\n",
    "    is_morph = True\n",
    "    def __init__(self, filterFunc, *args, **kwargs):\n",
    "        self.customFilter = filterFunc\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    def __eq__(self):\n",
    "        return (other\n",
    "                and self.__class__ is other.__class__)\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            if t.mode == 'query': # if called by query parser\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t\n",
    "            else: # == 'index' if called by indexer\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example1: Whoosh filter for NLTK's LancasterStemmer\n",
    "myFilter1 = RegexTokenizer() | CustomFilter(LancasterStemmer().stem)\n",
    "[token.text for token in myFilter1(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example2: Whoosh filter for NLTK's WordNetLemmatizer\n",
    "myFilter2 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize)\n",
    "[token.text for token in myFilter2(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example3: Whoosh filter for NLTK's WordNetLemmatizer for verbs\n",
    "myFilter3 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize, 'v')\n",
    "[token.text for token in myFilter3(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use myFilter1/2/3 as part of your Schema\n",
    "\n",
    "------------\n",
    "You can find details of other NLTK Stemmers and Lemmatizers here:\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
